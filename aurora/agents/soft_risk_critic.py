from typing import Dict, Any
from aurora.utils.data_models import Scenario
from aurora.models.openai_client import ask_gpt


VULNERABILITY_PATTERNS = [
    "lost my job",
    "no income",
    "cannot pay",
    "can't pay",
    "addiction",
    "gambling",
    "depression",
    "serious illness",
]


LLM_RISK_PROMPT = """
You are a compliance-oriented risk assessor for retail financial services.

Given a user message and an assistant response, you must:
1. Judge whether the interaction exposes the customer to financial or regulatory risk.
2. Produce a scalar risk score between 0.0 (no risk) and 1.0 (very high risk).
3. Assign a qualitative level: "LOW", "MEDIUM", or "HIGH".
4. Provide a short rationale in 2â€“3 sentences.

Respond ONLY with valid JSON in the following format:

{
  "risk_score": <float between 0.0 and 1.0>,
  "risk_level": "LOW" | "MEDIUM" | "HIGH",
  "rationale": "<short explanation>"
}

User message:
{user}

Assistant response:
{assistant}
"""


class SoftRiskCritiqueAgent:
    """
    Soft, LLM-based risk critic.

    Combines:
    - LLM-as-Judge scoring of interaction risk.
    - Lightweight heuristics for vulnerability cues.
    - Signals from the hard critic and retrieval meta-data.

    The output is a dictionary with:
    - "risk_score": float in [0, 1]
    - "risk_level": "LOW" | "MEDIUM" | "HIGH"
    - "rationale": human-readable explanation
    """

    def __init__(self, model: str = "gpt-4.1-mini") -> None:
        self.model = model

    def _contains_vulnerability(self, text: str) -> bool:
        txt = text.lower()
        return any(pat in txt for pat in VULNERABILITY_PATTERNS)

    def _level_from_score(self, score: float) -> str:
        if score >= 0.7:
            return "HIGH"
        if score >= 0.4:
            return "MEDIUM"
        return "LOW"

    # ------------------------------------------------------------------
    # API
    # ------------------------------------------------------------------

    def __call__(
        self,
        scenario: Scenario,
        retrieval_meta: Dict[str, Any] | None = None,
        hard_result: Dict[str, Any] | None = None,
    ) -> Dict[str, Any]:
        text_user = scenario.user_message
        text_assistant = scenario.assistant_response
        combined_text = f"{text_user} {text_assistant}"

        # ---------------- LLM-as-Judge (first pass) ----------------
        prompt = LLM_RISK_PROMPT.format(
            user=text_user,
            assistant=text_assistant,
        )
        try:
            raw = ask_gpt(prompt, model=self.model, max_tokens=300)
            llm_out = {}
            try:
                import json

                llm_out = json.loads(raw)
            except Exception:
                llm_out = {}
        except Exception:
            llm_out = {}

        base_score = float(llm_out.get("risk_score", 0.3))
        risk_score = max(0.0, min(1.0, base_score))
        rationale = llm_out.get("rationale", "").strip()

        # ---------------- Heuristics vulnerability and retrieval ----------------
        if self._contains_vulnerability(combined_text):
            # Encourage higher risk for vulnerable users if the model
            # has underestimated the risk.
            if risk_score < 0.7:
                risk_score = min(1.0, risk_score + 0.2)
                if rationale:
                    rationale += " Detected potential vulnerability in the user's situation."
                else:
                    rationale = "Detected potential vulnerability in the user's situation."

        if retrieval_meta is not None:
            if retrieval_meta.get("retrieval_failed", False):
                # Uncertain grounding should increase perceived risk.
                if risk_score < 0.6:
                    risk_score = 0.6
                extra = " Clause retrieval was low-confidence; treat this interaction conservatively."
                rationale = (rationale + " " + extra).strip()
            elif retrieval_meta.get("retrieval_confidence") is not None:
                conf = retrieval_meta["retrieval_confidence"]
                # Very high confidence combined with low risk can mildly
                # decrease the score.
                if conf > 0.9 and risk_score < 0.4:
                    risk_score = max(0.0, risk_score - 0.05)

        if hard_result is not None and hard_result.get("is_non_compliant", False):
            # Hard violations should never be rated as low risk.
            if risk_score < 0.7:
                risk_score = 0.75
                extra = " Hard-rule analysis indicates a likely breach."
                rationale = (rationale + " " + extra).strip()

        # ---------------- Second-pass refinement----------------
        # If the score lies in an ambiguous band, we allow the LLM a second opportunity to reassess with an explicitly conservative framing, then take the maximum of the two estimates.
        if 0.35 <= risk_score <= 0.65:
            conservative_prompt = (
                LLM_RISK_PROMPT
                + "\n\nYou MUST err on the side of caution and treat ambiguous cases as higher risk."
            ).format(user=text_user, assistant=text_assistant)
            try:
                raw2 = ask_gpt(conservative_prompt, model=self.model, max_tokens=300)
                import json

                llm_out2 = json.loads(raw2)
                score2 = float(llm_out2.get("risk_score", risk_score))
                score2 = max(0.0, min(1.0, score2))
                # Take the more conservative estimate.
                risk_score = max(risk_score, score2)
            except Exception:
                # If the second call fails keep the original score
                pass

        risk_score = max(0.0, min(1.0, risk_score))
        risk_level = self._level_from_score(risk_score)

        if not rationale:
            rationale = f"Estimated {risk_level} risk based on interaction content."

        return {
            "risk_score": risk_score,
            "risk_level": risk_level,
            "rationale": rationale,
        }
